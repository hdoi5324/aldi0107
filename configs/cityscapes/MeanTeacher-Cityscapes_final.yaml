# this file mimics the settings of using "Mean Teacher" off the shelf with
# the Adaptive Teacher / Unbiased Teacher repo, with our upgrades.
# that is:
# - batches are 1/3 weak source, 1/3 strong source, 1/3 target
# - bbox localization losses are disabled
# - random erasing is used as part of the strong augs
# - best standard burn-in checkpoint is used (baseline model trained w/o ema and strong augs)

_BASE_: "./Base-RCNN-FPN-Cityscapes_strongaug_ema.yaml"
MODEL:
  WEIGHTS: "outputs/cityscapes/cityscapes_baseline_strongaug_ema/model_final.pth"
EMA:
  ENABLED: True
DATASETS:
  UNLABELED: ("cityscapes_foggy_train",)
  BATCH_CONTENTS: ("labeled_weak", "labeled_strong", "unlabeled_strong")
  BATCH_RATIOS: (1,1,1)
DOMAIN_ADAPT:
  TEACHER:
    ENABLED: True
  DISTILL:
    # By default, disable localization losses (done by AT, UBT, MIC, ...)
    HARD_ROIH_CLS_ENABLED: True
    HARD_ROIH_REG_ENABLED: False
    HARD_OBJ_ENABLED: True
    HARD_RPN_REG_ENABLED: False
UMS:
  UNLABELED: None
SOLVER:
  STEPS: (14999,)
  MAX_ITER: 15000
  CHECKPOINT_PERIOD: 2000
  BACKWARD_AT_END: False
  BASE_LR: 0.03 # LR should be appropriate for IMS_PER_BATCH and MAX_ITERS
  IMS_PER_BATCH: 24 # Batch = 16 with 2 per GPU with 8 GPU = 0.02 LR
OUTPUT_DIR: "outputs/cityscapes/cityscapes_meanteacher_final/"
LOGGING:
  TAGS: "RCCN-FPN,MT_final"